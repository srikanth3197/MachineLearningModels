{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras_tuner import Hyperband\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predictive maintenance dataset\n",
    "Pred_main_data = pd.read_csv('ai4i2020.csv')\n",
    "Pred_main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'UDI' and 'Product ID' columns\n",
    "Pred_main_data = Pred_main_data.drop(columns=['UDI', 'Product ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'type' column categorical variables(low, mediam, high quality)\n",
    "Pred_main_data['Type'] = Pred_main_data['Type'].map({'L': 0, 'M': 1, 'H': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of the dataset\n",
    "Pred_main_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the missing values\n",
    "print(Pred_main_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of each column\n",
    "print(Pred_main_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the count of target variables\n",
    "Pred_main_data['Machine failure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical features\n",
    "numerical_features = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']\n",
    "\n",
    "# Plot histograms for numerical features\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(Pred_main_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for numerical features\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=Pred_main_data[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove outliers based on IQR\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Remove outliers from 'Rotational speed [rpm]' and 'Torque [Nm]'\n",
    "df = remove_outliers(Pred_main_data, 'Rotational speed [rpm]')\n",
    "df = remove_outliers(Pred_main_data, 'Torque [Nm]')\n",
    "\n",
    "# Verify the result\n",
    "print(\"Dataset shape after removing outliers:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for numerical features\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=Pred_main_data[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count plot for the 'Type' feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Type', data=Pred_main_data)\n",
    "plt.title('Distribution of Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=['L', 'M', 'H'])\n",
    "plt.show()\n",
    "\n",
    "# Plot count plot for the 'Machine failure' feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Machine failure', data=Pred_main_data)\n",
    "plt.title('Distribution of Machine Failure')\n",
    "plt.xlabel('Machine Failure')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = Pred_main_data.drop(columns=['Machine failure'])\n",
    "y = Pred_main_data['Machine failure']\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Combine the resampled features and target into a new DataFrame\n",
    "df_resampled = pd.concat([pd.DataFrame(X_res, columns=X.columns), pd.DataFrame(y_res, columns=['Machine failure'])], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled['Machine failure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns to normalize (excluding 'Machine failure' if it's your target variable)\n",
    "numerical_cols = df_resampled.drop(columns=['Machine failure']).columns\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the numerical columns\n",
    "df_resampled[numerical_cols] = scaler.fit_transform(df_resampled[numerical_cols])\n",
    "\n",
    "# Display the first few rows to check the normalized data\n",
    "df_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for 'Failure' (red) and 'No Failure' (blue)\n",
    "failure_colors = {0: 'blue', 1: 'red'}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot scatter plot with specific colors\n",
    "sns.scatterplot(x='Rotational speed [rpm]', y='Torque [Nm]', hue='Machine failure', data=df_resampled,\n",
    "                palette=failure_colors)\n",
    "\n",
    "plt.title('Rotational Speed vs Torque')\n",
    "plt.xlabel('Rotational Speed [rpm]')\n",
    "plt.ylabel('Torque [Nm]')\n",
    "\n",
    "# Create custom legend\n",
    "legend_labels = ['No Failure', 'Failure']\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in failure_colors.values()]\n",
    "plt.legend(legend_handles, legend_labels, title='Machine Failure')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_resampled.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_resampled.drop(columns=['Machine failure'])\n",
    "y = df_resampled['Machine failure']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the shapes of train and test sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prmain_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "Prmain_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_Prmain_rf = Prmain_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictive_maintenance_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Prints the classification report and visualizes the confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth target values.\n",
    "    - y_pred: Estimated targets as returned by a classifier.\n",
    "    - class_names: List of class names for labeling the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    class_names = ['No Failure', 'Failure']\n",
    "\n",
    "    # Print classification report\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Visualize confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_Prmain_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prmain_gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "Prmain_gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_Prmain_rf = Prmain_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_Prmain_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prmain_svc = SVC(random_state=42)\n",
    "\n",
    "Prmain_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_Prmain_svc = Prmain_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_Prmain_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prmain_knn = KNeighborsClassifier()\n",
    "\n",
    "Prmain_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_Prmain_knn = Prmain_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_Prmain_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN model\n",
    "Prmain_ann = Sequential()\n",
    "Prmain_ann.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer\n",
    "Prmain_ann.add(Dense(32, activation='relu'))  # Hidden layer\n",
    "Prmain_ann.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "Prmain_ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Fit the model on the training data\n",
    "history = Prmain_ann.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_prob_Prmain_ann = Prmain_ann.predict(X_test)\n",
    "y_pred_Prmain_ann = y_pred_prob_Prmain_ann.argmax(axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_Prmain_ann, target_names=['No Failure', 'Failure']))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_Prmain_ann)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Failure', 'Failure'], yticklabels=['No Failure', 'Failure'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix for ANN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparametertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "rf_param_grid = {\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "\n",
    "Prmain_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=Prmain_rf, param_grid=rf_param_grid,\n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Best Random Forest model\n",
    "Prmain_best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_Prmain_rf_hy = Prmain_best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_Prmain_rf_hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],          # Number of boosting stages to be run\n",
    "    'min_samples_split': [2, 5],          # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2],            # Minimum number of samples required at each leaf node\n",
    "    'subsample': [0.8, 0.9],              # Fraction of samples used for fitting the individual base learners\n",
    "     'learning_rate': [0.01, 0.1],        # Learning rate shrinks the contribution of each tree\n",
    "    'max_depth': [3, 4]                  # Maximum depth of the individual estimators\n",
    "}\n",
    "\n",
    "# Initialize a GradientBoostingClassifier\n",
    "Prmain_gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_gb = GridSearchCV(estimator=Prmain_gb, param_grid=gb_param_grid,\n",
    "                              cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params_gb = grid_search_gb.best_params_\n",
    "print(f\"Best hyperparameters: {best_params_gb}\")\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "Prmain_best_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_Prmain_gb_hy = Prmain_best_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_Prmain_gb_hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1],           # Regularization parameter\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    'kernel': ['rbf', 'linear', 'sigmoid']  # Specifies the kernel type to be used in the algorithm\n",
    "}\n",
    "\n",
    "# Initialize an SVC (Support Vector Classifier)\n",
    "Prmain_svc = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_svc = GridSearchCV(estimator=Prmain_svc, param_grid=svm_param_grid,\n",
    "                               cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_svc.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params_svc = grid_search_svc.best_params_\n",
    "print(f\"Best hyperparameters: {best_params_svc}\")\n",
    "\n",
    "# Best SVM model\n",
    "Prmain_best_svc = grid_search_svc.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_Prmain_svc_hy = Prmain_best_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_Prmain_svc_hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],  # Number of neighbors to use\n",
    "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "    'metric': ['euclidean', 'manhattan']  # Distance metric\n",
    "}\n",
    "\n",
    "# Initialize a KNeighborsClassifier\n",
    "Prmain_knn = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_knn = GridSearchCV(estimator=Prmain_knn, param_grid=knn_param_grid,\n",
    "                               cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params_knn = grid_search_knn.best_params_\n",
    "print(f\"Best hyperparameters: {best_params_knn}\")\n",
    "\n",
    "# Best KNN model\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn_Prmain_hy = best_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictive_maintenance_model(y_test, y_pred_knn_Prmain_hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units1', min_value=32, max_value=128, step=32), activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(units=hp.Int('units2', min_value=16, max_value=64, step=16), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=1,\n",
    "    directory='project_dir',\n",
    "    project_name='ann_hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Number of units in first Dense layer: {best_hyperparameters.get('units1')}\")\n",
    "print(f\"Number of units in second Dense layer: {best_hyperparameters.get('units2')}\")\n",
    "print(f\"Learning rate: {best_hyperparameters.get('learning_rate')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "history = best_model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_prob_Prmain_ann = best_model.predict(X_test)\n",
    "y_pred_Prmain_ann = y_pred_prob_Prmain_ann.argmax(axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_Prmain_ann, target_names=['No Failure', 'Failure']))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_Prmain_ann)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Failure', 'Failure'], yticklabels=['No Failure', 'Failure'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix for ANN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "fe_importances = Prmain_best_rf.feature_importances_\n",
    "\n",
    "# Convert importances to percentages\n",
    "importances_percentage = 100 * fe_importances / fe_importances.sum()\n",
    "\n",
    "# Create a DataFrame with features and their importance percentages\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance (%)': importances_percentage})\n",
    "\n",
    "# Sort the DataFrame by importance percentage in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance (%)', ascending=False)\n",
    "\n",
    "# Print the most important feature\n",
    "most_important_feature = importance_df.iloc[0]\n",
    "print(f\"The most important feature is '{most_important_feature['Feature']}' with an importance of {most_important_feature['Importance (%)']:.2f}%\")\n",
    "\n",
    "# Plot feature importances as percentage\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance (%)', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance from Random Forest Model (Percentage)')\n",
    "plt.xlabel('Importance (%)')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
